"""
linkedin_url_finder_mt.py
-----------------------------------------------------
Finds the most likely LinkedIn URL for each persona
using Tavily API and stores results in a DataFrame.

‚úÖ Optimized with multithreading for 5√ó faster speed.
‚úÖ Handles Tavily rate limits automatically.
‚úÖ Safe SQL updates back to PostgreSQL (Azure).
‚úÖ Works in Google Colab and GitHub Actions.

Author: Shubham Vishwas Purani
"""

import os
import sys
import json
import time
import random
import logging
import pandas as pd
from time import sleep
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.engine import URL
from tavily import TavilyClient
from concurrent.futures import ThreadPoolExecutor, as_completed

# ======================================================
# üîß CONFIGURATION
# ======================================================
load_dotenv()

PG_USER = os.getenv("PGUSER", "mathcoadmin")
PG_PASSWORD = os.getenv("PGPASSWORD", "Shubham@123")
PG_HOST = os.getenv("PGHOST", "psql-scout.postgres.database.azure.com")
PG_PORT = os.getenv("PGPORT", "5432")
PG_DB = os.getenv("PGDATABASE", "stakeholder360")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "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")

# ======================================================
# üß† LOGGER SETUP (Colab + GitHub Actions Safe)
# ======================================================
os.makedirs("logs", exist_ok=True)
IS_COLAB = "google.colab" in sys.modules
IS_GITHUB = os.getenv("GITHUB_ACTIONS") == "true"

handlers = [
    logging.StreamHandler(sys.stdout),
    logging.FileHandler("logs/linkedin_url_finder.log", mode="a")
]
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=handlers
)
logger = logging.getLogger(__name__)
for h in logger.handlers:
    h.flush = sys.stdout.flush

# ======================================================
# üß† DATABASE CONNECTION
# ======================================================
def get_engine():
    """Create SQLAlchemy engine for Azure PostgreSQL"""
    if not all([PG_USER, PG_PASSWORD, PG_HOST, PG_DB]):
        raise RuntimeError("‚ùå Database configuration incomplete. Check .env values.")

    url = URL.create(
        drivername="postgresql+psycopg2",
        username=PG_USER,
        password=PG_PASSWORD,
        host=PG_HOST,
        port=int(PG_PORT),
        database=PG_DB,
        query={"sslmode": "require"},  # Azure Postgres SSL
    )

    return create_engine(
        url,
        pool_size=10,
        max_overflow=20,
        pool_timeout=30,
        pool_recycle=1800,
        pool_pre_ping=True,
        echo=False,
    )

# ======================================================
# üì• LOAD DATA
# ======================================================
def load_centralize_df() -> pd.DataFrame:
    """Load the 'scout.centralize_db' table into a DataFrame"""
    try:
        engine = get_engine()
        sql = "SELECT * FROM scout.centralize_db"
        with engine.begin() as conn:
            df = pd.read_sql(sql, conn)
        logger.info(f"‚úÖ Loaded data successfully. Rows: {len(df)}")
        return df
    except Exception as e:
        logger.error(f"‚ùå Failed to load data: {e}")
        raise

# ======================================================
# üîç FETCH LINKEDIN URL (with retry + throttling)
# ======================================================
def fetch_single_url(client: TavilyClient, company_name: str, persona_name: str) -> dict:
    """Fetch a single LinkedIn URL with Tavily + retry + rate-limit handling"""
    max_retries = 4
    base_delay = 2  # seconds

    for attempt in range(1, max_retries + 1):
        try:
            query = (
                f'Respond only with LinkedIn profile URL or "No URL" '
                f'for the persona {persona_name} from {company_name}.'
            )
            response = client.search(
                query=query,
                include_answer="advanced",
                max_results=3,
            )
            return {
                "company_name": company_name,
                "persona_name": persona_name,
                "linkedin_url": response.get("answer"),
            }

        except Exception as e:
            err_msg = str(e)
            if "blocked due to excessive requests" in err_msg or "429" in err_msg:
                wait_time = base_delay * attempt + random.uniform(0, 1)
                logger.warning(
                    f"‚ö†Ô∏è Rate limited (attempt {attempt}) for {persona_name}. "
                    f"Sleeping {wait_time:.1f}s before retry..."
                )
                sleep(wait_time)
                continue
            logger.error(f"‚ùå Error fetching {persona_name}: {err_msg}")
            sleep(1)
            continue

    return {
        "company_name": company_name,
        "persona_name": persona_name,
        "linkedin_url": "Error: Tavily rate limit or failed after retries",
    }

# ======================================================
# üß© MULTITHREADED FETCH
# ======================================================
def fetch_linkedin_urls(df: pd.DataFrame, limit: int = 100, max_workers: int = 8) -> pd.DataFrame:
    """Fetch LinkedIn URLs using Tavily API in parallel"""
    df = df.copy()
    df = df[df["linkedin_url"].isna() | (df["linkedin_url"] == "NaN")]

    if df.empty:
        logger.warning("‚ö†Ô∏è No records with missing LinkedIn URLs found.")
        return pd.DataFrame()

    df = df.head(limit)
    client = TavilyClient(TAVILY_API_KEY)
    logger.info(f"üîç Fetching {len(df)} personas using {max_workers} threads...")

    results = []
    start = time.time()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_persona = {
            executor.submit(fetch_single_url, client, row["account"], row["client_name"]): row
            for _, row in df.iterrows()
        }

        for idx, future in enumerate(as_completed(future_to_persona), start=1):
            result = future.result()
            results.append(result)
            logger.info(f"[{idx}/{len(df)}] {result['persona_name']} ‚Üí {result['linkedin_url']}")
            sleep(0.2)  # small global delay to keep API safe

    elapsed = time.time() - start
    logger.info(f"‚úÖ Completed {len(df)} lookups in {elapsed:.2f}s")
    return pd.json_normalize(results)

# ======================================================
# üß± UPDATE DATABASE
# ======================================================
def update_linkedin_urls(final_df: pd.DataFrame):
    """Update linkedin_url column in scout.centralize_db"""
    if final_df.empty:
        logger.warning("‚ö†Ô∏è No records to update.")
        return

    engine = get_engine()
    updated_count = 0
    logger.info("üß© Starting database update process...")

    update_sql = text("""
        UPDATE scout.centralize_db
        SET linkedin_url = :linkedin_url
        WHERE client_name = :persona_name
          AND account = :company_name;
    """)

    with engine.begin() as conn:
        for idx, row in final_df.iterrows():
            try:
                params = {
                    "linkedin_url": row["linkedin_url"],
                    "persona_name": row["persona_name"],
                    "company_name": row["company_name"],
                }
                conn.execute(update_sql, params)
                updated_count += 1
                if idx % 50 == 0:
                    logger.info(f"üü¢ Updated {idx} records so far...")
            except Exception as e:
                logger.error(f"‚ùå Failed to update ({row['persona_name']}, {row['company_name']}): {e}")

    logger.info(f"‚úÖ Database update complete. Total updated: {updated_count} records.")

# ======================================================
# ‚ö° MAIN
# ======================================================
def main():
    start_time = time.time()
    df = load_centralize_df()
    final_df = fetch_linkedin_urls(df, limit=1000, max_workers=8)
    logger.info(f"Final results: {final_df.shape}")

    # üß± Update DB
    update_linkedin_urls(final_df)

    elapsed = time.time() - start_time
    logger.info(f"‚è±Ô∏è Total time: {elapsed:.2f}s")
    print(f"‚è±Ô∏è Total time: {elapsed:.2f}s")
    return final_df

# ======================================================
# üß© ENTRY POINT
# ======================================================
if __name__ == "__main__":
    final_persona_linkedin_url_df = main()
    print("\n‚úÖ Final Output Preview:")
    print(final_persona_linkedin_url_df.head())
